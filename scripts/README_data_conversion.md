# Data Conversion Utilities for EpiBERT

This directory contains comprehensive utilities for converting data to PyTorch-compatible formats for EpiBERT training.

## Overview

The EpiBERT project now supports two main data conversion pathways:

1. **TFRecord to HDF5 Conversion**: Convert existing TensorFlow TFRecord files to PyTorch-compatible HDF5 format
2. **Direct PyTorch Dataset Generation**: Generate PyTorch datasets directly from genomic data files

Both approaches produce HDF5 files that are fully compatible with the PyTorch Lightning training pipeline.

## Utilities

### 1. TFRecord to HDF5 Converter (`convert_tfrecord_to_hdf5.py`)

Converts TensorFlow TFRecord files generated by the original EpiBERT data processing pipeline to HDF5 format.

**Features:**
- Supports all standard EpiBERT TFRecord fields (sequence, ATAC, peaks, motif activity)
- Batch conversion of multiple files
- Parallel processing
- Comprehensive validation
- One-hot encoding of DNA sequences
- Automatic data type conversion and reshaping

**Usage:**

```bash
# Convert single TFRecord file
python scripts/convert_tfrecord_to_hdf5.py --input data.tfrecord --output data.h5

# Convert multiple TFRecord files
python scripts/convert_tfrecord_to_hdf5.py --input-dir tfrecords/ --output-dir converted/ --workers 4

# Convert with custom parameters
python scripts/convert_tfrecord_to_hdf5.py --input data.tfrecord --output data.h5 \
  --sequence-length 524288 --output-length 4096 --motif-features 693

# Validate converted file
python scripts/convert_tfrecord_to_hdf5.py --validate converted_data.h5

# Create dataset manifest
python scripts/convert_tfrecord_to_hdf5.py --input-dir tfrecords/ --output-dir converted/ \
  --create-manifest --manifest-name my_dataset
```

### 2. PyTorch Dataset Generator (`generate_pytorch_dataset.py`)

Generates PyTorch-compatible HDF5 datasets directly from genomic data files, bypassing TFRecord format entirely.

**Features:**
- Direct processing from BED/bedGraph/BigWig files
- FASTA sequence extraction and one-hot encoding
- Peak center extraction from BED files
- Motif activity score integration
- Custom genomic region support
- Multi-sample batch processing
- Parallel processing with shared regions

**Usage:**

```bash
# Generate dataset from single sample
python scripts/generate_pytorch_dataset.py --sample-id sample1 \
  --atac-file atac.bedgraph --rampage-file rampage.bedgraph \
  --peaks-file peaks.bed --output sample1_dataset.h5 \
  --genome-fasta hg38.fa --genome-sizes hg38.sizes

# Generate datasets from sample manifest
python scripts/generate_pytorch_dataset.py --samples-config samples.yaml \
  --output-dir datasets/ --genome-fasta hg38.fa --genome-sizes hg38.sizes \
  --workers 4

# Generate with custom regions
python scripts/generate_pytorch_dataset.py --samples-config samples.yaml \
  --output-dir datasets/ --genome-fasta hg38.fa --genome-sizes hg38.sizes \
  --regions-bed custom_regions.bed
```

## Sample Configuration Files

### TFRecord Conversion Manifest

When using `--create-manifest`, the converter creates a YAML manifest file:

```yaml
dataset_name: converted_dataset
total_samples: 10
total_examples: 50000
data_format: hdf5_converted_from_tfrecord
created_by: EpiBERT_tfrecord_converter
samples:
  - sample_id: sample1
    atac_file: /path/to/converted_sample1.h5
    rampage_file: /path/to/converted_sample1.h5
    n_examples: 5000
    converted_from: /path/to/original_sample1.tfrecord
    file_format: hdf5
```

### PyTorch Dataset Sample Configuration

For batch processing with the PyTorch dataset generator, create a samples configuration file:

```yaml
# samples_config.yaml
samples:
  - sample_id: sample1
    atac_file: /path/to/sample1_atac.bedgraph
    rampage_file: /path/to/sample1_rampage.bedgraph
    peaks_file: /path/to/sample1_peaks.bed
    condition: treatment
    
  - sample_id: sample2
    atac_file: /path/to/sample2_atac.bedgraph
    rampage_file: /path/to/sample2_rampage.bedgraph
    peaks_file: /path/to/sample2_peaks.bed
    condition: control
    
  - sample_id: sample3
    atac_file: /path/to/sample3_atac.bw  # BigWig files also supported
    rampage_file: /path/to/sample3_rampage.bw
    peaks_file: /path/to/sample3_peaks.bed
    condition: treatment
```

### Custom Regions File

To process specific genomic regions, create a BED file:

```
# custom_regions.bed
chr1	1000000	1524288	region_1
chr1	2000000	2524288	region_2
chr2	500000	1024288	region_3
```

## Data Format Specifications

### Input Data Formats

**Supported Signal File Formats:**
- **bedGraph**: Tab-separated with columns: chrom, start, end, value
- **BigWig**: Binary format for continuous signals
- **BED**: For peak files with columns: chrom, start, end, [name]

**Required Genome Files:**
- **FASTA**: Reference genome sequences
- **Chromosome Sizes**: Tab-separated file with columns: chrom, size

### Output HDF5 Format

Generated HDF5 files contain the following datasets:

```python
with h5py.File('dataset.h5', 'r') as f:
    sequences = f['sequences'][:]          # Shape: (N, 4, seq_len), one-hot encoded DNA
    atac_profiles = f['atac_profiles'][:]  # Shape: (N, profile_len), ATAC-seq signal
    rampage_profiles = f['rampage_profiles'][:]  # Shape: (N, profile_len), RAMPAGE-seq signal
    motif_activities = f['motif_activities'][:]  # Shape: (N, 693), motif enrichment scores
    peaks_centers = f['peaks_centers'][:]  # Shape: (N, max_peaks), peak center positions
    
    # Metadata
    sample_id = f.attrs['sample_id']
    n_regions = f.attrs['n_regions']
    sequence_length = f.attrs['sequence_length']
    output_length = f.attrs['output_length']
```

## Integration with PyTorch Lightning

The generated HDF5 files are fully compatible with the EpiBERT PyTorch Lightning data modules:

```python
from lightning_transfer.data_module import create_data_module

# Single dataset
data_module = create_data_module(
    data_dir="/path/to/hdf5/files",
    batch_size=8,
    num_workers=4
)

# Multi-sample dataset with manifest
data_module = create_data_module(
    manifest_file="/path/to/dataset_manifest.yaml",
    batch_size=8,
    num_workers=4,
    use_paired_dataset=True
)
```

## Performance Considerations

### TFRecord Conversion
- **Memory Usage**: Processes one file at a time to manage memory
- **Parallelization**: Use `--workers` for batch conversion
- **Storage**: HDF5 files are typically 20-30% smaller than TFRecord due to compression

### PyTorch Dataset Generation
- **I/O Optimization**: Use SSD storage for input files when possible
- **Shared Regions**: Enable for consistent regions across samples (default)
- **Parallel Processing**: Scales well with number of CPU cores
- **Memory Management**: Processes regions incrementally to handle large datasets

### Recommended Workflow

1. **For Existing TFRecord Files:**
   ```bash
   # Convert all TFRecord files
   python scripts/convert_tfrecord_to_hdf5.py --input-dir tfrecords/ \
     --output-dir pytorch_data/ --workers 8 --create-manifest
   
   # Use converted data for training
   python scripts/train_model.py --data-manifest pytorch_data/dataset_manifest.yaml
   ```

2. **For New Data Processing:**
   ```bash
   # Create sample configuration
   # Edit samples_config.yaml with your data file paths
   
   # Generate PyTorch datasets directly
   python scripts/generate_pytorch_dataset.py --samples-config samples_config.yaml \
     --output-dir pytorch_data/ --genome-fasta hg38.fa --genome-sizes hg38.sizes \
     --workers 8 --create-manifest
   
   # Use generated data for training
   python scripts/train_model.py --data-manifest pytorch_data/pytorch_dataset_manifest.yaml
   ```

## Testing

A comprehensive test suite is available to validate the conversion utilities:

```bash
# Run all conversion tests
python scripts/test_data_conversion.py --verbose

# Test with custom directory
python scripts/test_data_conversion.py --test-dir /path/to/test/dir --keep-files
```

The test suite validates:
- TFRecord parsing and conversion
- HDF5 file generation and validation
- PyTorch Lightning data module integration
- Batch processing functionality
- Data integrity and format compliance

## Troubleshooting

### Common Issues

1. **Missing Dependencies:**
   ```bash
   pip install numpy h5py pandas tensorflow pysam pyBigWig pyyaml tqdm
   ```

2. **Memory Issues with Large Files:**
   - Reduce `--workers` parameter
   - Process files individually
   - Use smaller regions or reduce sequence length

3. **FASTA Index Missing:**
   ```bash
   samtools faidx genome.fa
   ```

4. **BigWig File Access Issues:**
   - Ensure files are properly formatted
   - Check chromosome naming consistency

5. **TFRecord Compatibility:**
   - Verify TFRecord files contain expected features
   - Check data types match EpiBERT format

### Performance Optimization

1. **For Large Datasets:**
   - Use SSD storage for better I/O performance
   - Increase `--workers` based on available CPU cores
   - Consider processing subsets of data in batches

2. **Memory Optimization:**
   - Reduce sequence/output lengths for testing
   - Use `--no-shared-regions` for memory-constrained systems
   - Process files sequentially if parallel processing fails

3. **Storage Optimization:**
   - HDF5 compression is enabled by default
   - Clean up intermediate files after conversion
   - Use symbolic links for shared reference files

## Advanced Usage

### Custom Data Processing Pipeline

For advanced users who want to integrate the converters into custom pipelines:

```python
from scripts.convert_tfrecord_to_hdf5 import TFRecordToHDF5Converter
from scripts.generate_pytorch_dataset import PyTorchDatasetGenerator

# Initialize converters with custom parameters
tfrecord_converter = TFRecordToHDF5Converter(
    sequence_length=524288,
    output_length=4096,
    motif_features=693,
    one_hot_encode=True
)

pytorch_generator = PyTorchDatasetGenerator(
    genome_fasta="hg38.fa",
    genome_sizes="hg38.sizes",
    sequence_length=524288,
    output_length=4096,
    resolution=128
)

# Custom processing workflows
# ... your custom code here ...
```

### Integration with Existing Pipelines

The utilities can be integrated into existing data processing workflows:

```bash
#!/bin/bash
# example_pipeline.sh

# Step 1: Process raw data to create TFRecords (existing pipeline)
python data_processing/write_TF_records/data_processing_seq_to_atac_globalacc.py ...

# Step 2: Convert TFRecords to PyTorch format
python scripts/convert_tfrecord_to_hdf5.py --input-dir tfrecords/ --output-dir pytorch_data/

# Step 3: Train with PyTorch Lightning
python scripts/train_model.py --data-dir pytorch_data/
```