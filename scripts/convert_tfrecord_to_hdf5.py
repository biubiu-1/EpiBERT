#!/usr/bin/env python3
"""
TFRecord to HDF5 Converter for EpiBERT

Converts TensorFlow TFRecord files generated by the original EpiBERT data processing
pipeline to HDF5 format compatible with PyTorch Lightning training.

Supports batch conversion of multiple TFRecord files and handles all standard
EpiBERT data fields including sequences, ATAC profiles, peak centers, and motif activities.
"""

import os
import sys
import argparse
import numpy as np
import h5py
import tensorflow as tf
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
from tqdm import tqdm

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Disable GPU for TensorFlow (we're just reading data)
tf.config.set_visible_devices([], 'GPU')


class TFRecordToHDF5Converter:
    """
    Converts TFRecord files to HDF5 format for PyTorch Lightning training
    
    Handles the standard EpiBERT TFRecord format with sequence, ATAC, peaks, and motif data.
    """
    
    def __init__(self, 
                 sequence_length: int = 524288,
                 output_length: int = 4096,
                 motif_features: int = 693,
                 one_hot_encode: bool = True):
        """
        Initialize converter
        
        Args:
            sequence_length: Expected sequence length
            output_length: Expected output profile length
            motif_features: Number of motif features
            one_hot_encode: Whether to one-hot encode DNA sequences
        """
        self.sequence_length = sequence_length
        self.output_length = output_length
        self.motif_features = motif_features
        self.one_hot_encode = one_hot_encode
        
        # Setup TensorFlow feature map for parsing
        self.feature_map = {
            'sequence': tf.io.FixedLenFeature([], tf.string),
            'atac': tf.io.FixedLenFeature([], tf.string),
            'peaks_center': tf.io.FixedLenFeature([], tf.string),
            'motif_activity': tf.io.FixedLenFeature([], tf.string)
        }
        
    def one_hot_encode_sequence(self, sequence_str: str) -> np.ndarray:
        """Convert DNA sequence string to one-hot encoding"""
        
        # Create mapping
        base_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4}
        
        # Convert sequence to indices
        sequence_upper = sequence_str.upper()
        indices = [base_to_idx.get(base, 4) for base in sequence_upper]
        
        # Create one-hot encoding (4 channels for A, C, G, T)
        one_hot = np.zeros((4, len(indices)), dtype=np.float32)
        
        for i, idx in enumerate(indices):
            if idx < 4:  # Valid base (not N)
                one_hot[idx, i] = 1.0
                
        return one_hot
        
    def parse_tfrecord_example(self, example_proto: bytes) -> Dict[str, np.ndarray]:
        """Parse a single TFRecord example"""
        
        try:
            # Parse the example
            parsed_example = tf.io.parse_single_example(example_proto, self.feature_map)
            
            # Extract sequence
            sequence_str = parsed_example['sequence'].numpy().decode('utf-8')
            if self.one_hot_encode:
                sequence = self.one_hot_encode_sequence(sequence_str)
            else:
                sequence = np.array(list(sequence_str.upper()))
                
            # Extract ATAC profile
            atac_tensor = tf.io.parse_tensor(parsed_example['atac'], out_type=tf.float16)
            atac = tf.cast(atac_tensor, tf.float32).numpy()
            
            # Extract peaks centers
            peaks_tensor = tf.io.parse_tensor(parsed_example['peaks_center'], out_type=tf.int32)
            peaks_center = peaks_tensor.numpy()
            
            # Extract motif activity
            motif_tensor = tf.io.parse_tensor(parsed_example['motif_activity'], out_type=tf.float16)
            motif_activity = tf.cast(motif_tensor, tf.float32).numpy()
            
            # Validate and reshape data
            if self.one_hot_encode and sequence.shape != (4, self.sequence_length):
                # Pad or truncate sequence to expected length
                if sequence.shape[1] < self.sequence_length:
                    padding = np.zeros((4, self.sequence_length - sequence.shape[1]), dtype=np.float32)
                    sequence = np.concatenate([sequence, padding], axis=1)
                elif sequence.shape[1] > self.sequence_length:
                    sequence = sequence[:, :self.sequence_length]
                    
            # Ensure ATAC profile is correct shape
            if atac.ndim == 2 and atac.shape[1] == 1:
                atac = atac.squeeze(1)  # Remove singleton dimension
                
            # Ensure peaks_center is correct shape
            if peaks_center.ndim == 1 and len(peaks_center) != self.output_length:
                if len(peaks_center) < self.output_length:
                    padding = np.zeros(self.output_length - len(peaks_center), dtype=np.int32)
                    peaks_center = np.concatenate([peaks_center, padding])
                else:
                    peaks_center = peaks_center[:self.output_length]
                    
            # Ensure motif activity is correct shape
            if motif_activity.ndim == 1 and len(motif_activity) != self.motif_features:
                if len(motif_activity) < self.motif_features:
                    padding = np.zeros(self.motif_features - len(motif_activity), dtype=np.float32)
                    motif_activity = np.concatenate([motif_activity, padding])
                else:
                    motif_activity = motif_activity[:self.motif_features]
                    
            return {
                'sequence': sequence,
                'atac_profile': atac,
                'peaks_center': peaks_center,
                'motif_activity': motif_activity
            }
            
        except Exception as e:
            logger.warning(f"Error parsing TFRecord example: {e}")
            return None
            
    def convert_tfrecord_file(self, 
                             tfrecord_path: str, 
                             output_path: str,
                             sample_id: Optional[str] = None) -> str:
        """
        Convert a single TFRecord file to HDF5 format
        
        Args:
            tfrecord_path: Path to input TFRecord file
            output_path: Path for output HDF5 file
            sample_id: Optional sample identifier
            
        Returns:
            Path to created HDF5 file
        """
        
        tfrecord_path = Path(tfrecord_path)
        output_path = Path(output_path)
        
        if not tfrecord_path.exists():
            raise FileNotFoundError(f"TFRecord file not found: {tfrecord_path}")
            
        logger.info(f"Converting {tfrecord_path} to {output_path}")
        
        # Create output directory
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Read TFRecord file
        dataset = tf.data.TFRecordDataset(str(tfrecord_path))
        
        # Parse all examples
        sequences = []
        atac_profiles = []
        peaks_centers = []
        motif_activities = []
        
        total_examples = 0
        valid_examples = 0
        
        for example_proto in tqdm(dataset, desc="Processing examples"):
            total_examples += 1
            
            parsed_data = self.parse_tfrecord_example(example_proto.numpy())
            
            if parsed_data is not None:
                sequences.append(parsed_data['sequence'])
                atac_profiles.append(parsed_data['atac_profile'])
                peaks_centers.append(parsed_data['peaks_center'])
                motif_activities.append(parsed_data['motif_activity'])
                valid_examples += 1
                
        if valid_examples == 0:
            raise ValueError(f"No valid examples found in {tfrecord_path}")
            
        logger.info(f"Parsed {valid_examples}/{total_examples} examples successfully")
        
        # Convert to numpy arrays
        sequences = np.array(sequences)
        atac_profiles = np.array(atac_profiles)
        peaks_centers = np.array(peaks_centers)
        motif_activities = np.array(motif_activities)
        
        # Save to HDF5
        with h5py.File(output_path, 'w') as f:
            # Save main datasets
            f.create_dataset('sequences', data=sequences, compression='gzip', chunks=True)
            f.create_dataset('atac_profiles', data=atac_profiles, compression='gzip', chunks=True)
            f.create_dataset('peaks_centers', data=peaks_centers, compression='gzip', chunks=True)
            f.create_dataset('motif_activities', data=motif_activities, compression='gzip', chunks=True)
            
            # Save metadata
            f.attrs['sample_id'] = sample_id or tfrecord_path.stem
            f.attrs['n_examples'] = valid_examples
            f.attrs['sequence_length'] = self.sequence_length
            f.attrs['output_length'] = self.output_length
            f.attrs['motif_features'] = self.motif_features
            f.attrs['one_hot_encoded'] = self.one_hot_encode
            f.attrs['converted_from'] = str(tfrecord_path)
            f.attrs['created_by'] = 'EpiBERT_tfrecord_converter'
            
            # Save shapes for validation
            f.attrs['sequences_shape'] = sequences.shape
            f.attrs['atac_profiles_shape'] = atac_profiles.shape
            f.attrs['peaks_centers_shape'] = peaks_centers.shape
            f.attrs['motif_activities_shape'] = motif_activities.shape
            
        logger.info(f"Successfully converted {valid_examples} examples to {output_path}")
        return str(output_path)
        
    def convert_multiple_tfrecords(self,
                                  tfrecord_files: List[str],
                                  output_dir: str,
                                  num_workers: int = 4,
                                  prefix: str = "converted") -> List[str]:
        """
        Convert multiple TFRecord files in parallel
        
        Args:
            tfrecord_files: List of TFRecord file paths
            output_dir: Output directory for HDF5 files
            num_workers: Number of parallel workers
            prefix: Prefix for output files
            
        Returns:
            List of created HDF5 file paths
        """
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Converting {len(tfrecord_files)} TFRecord files using {num_workers} workers")
        
        # Prepare conversion tasks
        conversion_tasks = []
        for i, tfrecord_file in enumerate(tfrecord_files):
            tfrecord_path = Path(tfrecord_file)
            output_filename = f"{prefix}_{tfrecord_path.stem}.h5"
            output_path = output_dir / output_filename
            
            conversion_tasks.append((
                str(tfrecord_path),
                str(output_path),
                tfrecord_path.stem
            ))
            
        # Execute conversions in parallel
        converted_files = []
        
        if num_workers == 1:
            # Single-threaded processing
            for tfrecord_path, output_path, sample_id in conversion_tasks:
                try:
                    converted_file = self.convert_tfrecord_file(
                        tfrecord_path, output_path, sample_id
                    )
                    converted_files.append(converted_file)
                except Exception as e:
                    logger.error(f"Error converting {tfrecord_path}: {e}")
        else:
            # Multi-threaded processing
            with ProcessPoolExecutor(max_workers=num_workers) as executor:
                # Submit all tasks
                future_to_task = {}
                for tfrecord_path, output_path, sample_id in conversion_tasks:
                    future = executor.submit(
                        self.convert_tfrecord_file,
                        tfrecord_path, output_path, sample_id
                    )
                    future_to_task[future] = (tfrecord_path, output_path, sample_id)
                    
                # Collect results
                for future in as_completed(future_to_task):
                    tfrecord_path, output_path, sample_id = future_to_task[future]
                    try:
                        converted_file = future.result()
                        converted_files.append(converted_file)
                        logger.info(f"Completed conversion: {Path(tfrecord_path).name}")
                    except Exception as e:
                        logger.error(f"Error converting {tfrecord_path}: {e}")
                        
        logger.info(f"Successfully converted {len(converted_files)}/{len(tfrecord_files)} files")
        return converted_files
        

def validate_converted_hdf5(hdf5_path: str) -> Dict[str, Any]:
    """
    Validate a converted HDF5 file and return information
    
    Args:
        hdf5_path: Path to HDF5 file
        
    Returns:
        Dictionary with validation results and file information
    """
    
    info = {
        'valid': False,
        'path': hdf5_path,
        'error': None,
        'n_examples': 0,
        'datasets': {},
        'attributes': {}
    }
    
    try:
        with h5py.File(hdf5_path, 'r') as f:
            # Check required datasets
            required_datasets = ['sequences', 'atac_profiles', 'peaks_centers', 'motif_activities']
            missing_datasets = []
            
            for dataset_name in required_datasets:
                if dataset_name in f:
                    info['datasets'][dataset_name] = {
                        'shape': f[dataset_name].shape,
                        'dtype': str(f[dataset_name].dtype)
                    }
                else:
                    missing_datasets.append(dataset_name)
                    
            if missing_datasets:
                info['error'] = f"Missing required datasets: {missing_datasets}"
                return info
                
            # Check data consistency
            n_examples = f['sequences'].shape[0]
            for dataset_name in required_datasets:
                if f[dataset_name].shape[0] != n_examples:
                    info['error'] = f"Inconsistent number of examples across datasets"
                    return info
                    
            info['n_examples'] = n_examples
            
            # Get attributes
            for attr_name in f.attrs.keys():
                info['attributes'][attr_name] = f.attrs[attr_name]
                
            info['valid'] = True
            
    except Exception as e:
        info['error'] = str(e)
        
    return info


def create_dataset_manifest(hdf5_files: List[str], 
                           output_path: str,
                           dataset_name: str = "converted_dataset") -> str:
    """
    Create a dataset manifest file for converted HDF5 files
    
    Args:
        hdf5_files: List of HDF5 file paths
        output_path: Path for output manifest file
        dataset_name: Name for the dataset
        
    Returns:
        Path to created manifest file
    """
    
    import yaml
    
    logger.info(f"Creating dataset manifest for {len(hdf5_files)} files")
    
    # Validate files and collect information
    samples = []
    total_examples = 0
    
    for hdf5_file in hdf5_files:
        validation_info = validate_converted_hdf5(hdf5_file)
        
        if validation_info['valid']:
            sample_info = {
                'sample_id': validation_info['attributes'].get('sample_id', Path(hdf5_file).stem),
                'atac_file': str(hdf5_file),
                'rampage_file': str(hdf5_file),  # Same file contains both
                'n_examples': validation_info['n_examples'],
                'converted_from': validation_info['attributes'].get('converted_from', 'unknown'),
                'file_format': 'hdf5'
            }
            samples.append(sample_info)
            total_examples += validation_info['n_examples']
        else:
            logger.warning(f"Skipping invalid file {hdf5_file}: {validation_info['error']}")
            
    # Create manifest
    manifest_data = {
        'dataset_name': dataset_name,
        'total_samples': len(samples),
        'total_examples': total_examples,
        'data_format': 'hdf5_converted_from_tfrecord',
        'created_by': 'EpiBERT_tfrecord_converter',
        'samples': samples
    }
    
    # Save manifest
    with open(output_path, 'w') as f:
        yaml.dump(manifest_data, f, default_flow_style=False, indent=2)
        
    logger.info(f"Created manifest with {len(samples)} samples: {output_path}")
    return output_path


def main():
    """Main CLI interface for TFRecord to HDF5 conversion"""
    
    parser = argparse.ArgumentParser(
        description="Convert EpiBERT TFRecord files to HDF5 format for PyTorch Lightning",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Convert single TFRecord file
  python convert_tfrecord_to_hdf5.py --input data.tfrecord --output data.h5

  # Convert multiple TFRecord files
  python convert_tfrecord_to_hdf5.py --input-dir tfrecords/ --output-dir converted/ --workers 4

  # Convert with custom parameters
  python convert_tfrecord_to_hdf5.py --input data.tfrecord --output data.h5 --sequence-length 524288 --output-length 4096

  # Validate converted file
  python convert_tfrecord_to_hdf5.py --validate converted_data.h5
        """
    )
    
    # Input/output arguments
    parser.add_argument('--input', '-i',
                       help='Input TFRecord file path')
    parser.add_argument('--input-dir',
                       help='Directory containing TFRecord files')
    parser.add_argument('--output', '-o',
                       help='Output HDF5 file path (for single file conversion)')
    parser.add_argument('--output-dir',
                       help='Output directory for HDF5 files (for batch conversion)')
    
    # Conversion parameters
    parser.add_argument('--sequence-length',
                       type=int,
                       default=524288,
                       help='Expected sequence length')
    parser.add_argument('--output-length',
                       type=int,
                       default=4096,
                       help='Expected output profile length')
    parser.add_argument('--motif-features',
                       type=int,
                       default=693,
                       help='Number of motif features')
    parser.add_argument('--no-one-hot',
                       action='store_true',
                       help='Do not one-hot encode sequences')
    
    # Processing parameters
    parser.add_argument('--workers',
                       type=int,
                       default=4,
                       help='Number of parallel workers')
    parser.add_argument('--prefix',
                       default='converted',
                       help='Prefix for output files')
    
    # Validation
    parser.add_argument('--validate',
                       help='Validate an HDF5 file and print information')
    
    # Manifest creation
    parser.add_argument('--create-manifest',
                       action='store_true',
                       help='Create dataset manifest for converted files')
    parser.add_argument('--manifest-name',
                       default='converted_dataset',
                       help='Name for the dataset in manifest')
    
    args = parser.parse_args()
    
    # Handle validation mode
    if args.validate:
        info = validate_converted_hdf5(args.validate)
        print(f"\nValidation results for {args.validate}:")
        print(f"Valid: {info['valid']}")
        if info['error']:
            print(f"Error: {info['error']}")
        else:
            print(f"Number of examples: {info['n_examples']}")
            print("\nDatasets:")
            for name, details in info['datasets'].items():
                print(f"  {name}: {details['shape']} ({details['dtype']})")
            print("\nAttributes:")
            for name, value in info['attributes'].items():
                print(f"  {name}: {value}")
        return
        
    # Initialize converter
    converter = TFRecordToHDF5Converter(
        sequence_length=args.sequence_length,
        output_length=args.output_length,
        motif_features=args.motif_features,
        one_hot_encode=not args.no_one_hot
    )
    
    converted_files = []
    
    if args.input:
        # Single file conversion
        if not args.output:
            args.output = Path(args.input).stem + '.h5'
            
        converted_file = converter.convert_tfrecord_file(
            args.input, args.output
        )
        converted_files = [converted_file]
        print(f"Conversion complete: {converted_file}")
        
    elif args.input_dir:
        # Batch conversion
        if not args.output_dir:
            print("Error: --output-dir required for batch conversion")
            sys.exit(1)
            
        # Find TFRecord files
        input_dir = Path(args.input_dir)
        tfrecord_files = []
        
        for pattern in ['*.tfrecord', '*.tfrecords', '*.tf']:
            tfrecord_files.extend(input_dir.glob(pattern))
            
        if not tfrecord_files:
            print(f"No TFRecord files found in {input_dir}")
            sys.exit(1)
            
        print(f"Found {len(tfrecord_files)} TFRecord files")
        
        converted_files = converter.convert_multiple_tfrecords(
            [str(f) for f in tfrecord_files],
            args.output_dir,
            num_workers=args.workers,
            prefix=args.prefix
        )
        
        print(f"Batch conversion complete: {len(converted_files)} files converted")
        
    else:
        print("Error: Either --input or --input-dir required")
        parser.print_help()
        sys.exit(1)
        
    # Create manifest if requested
    if args.create_manifest and converted_files:
        manifest_path = Path(args.output_dir or '.') / 'dataset_manifest.yaml'
        create_dataset_manifest(
            converted_files,
            str(manifest_path),
            args.manifest_name
        )
        print(f"Dataset manifest created: {manifest_path}")
        
    print("All conversions completed successfully!")


if __name__ == "__main__":
    main()