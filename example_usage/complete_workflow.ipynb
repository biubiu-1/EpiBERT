{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EpiBERT Complete Workflow Example\n",
    "\n",
    "This notebook demonstrates the complete EpiBERT workflow from data processing to model evaluation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The EpiBERT workflow consists of:\n",
    "1. **Environment Setup**: Install dependencies and validate tools\n",
    "2. **Data Processing**: Convert BAM files to model-ready format\n",
    "3. **Model Training**: Train pretraining or fine-tuning models\n",
    "4. **Model Evaluation**: Comprehensive performance assessment\n",
    "\n",
    "This example uses the **PyTorch Lightning implementation** which provides:\n",
    "- Exact parameter matching with original TensorFlow models\n",
    "- Modern training features (mixed precision, gradient clipping)\n",
    "- Better hardware utilization\n",
    "- Simplified training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add repository root to path\n",
    "repo_root = Path.cwd().parent if 'example_usage' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(f\"Repository root: {repo_root}\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate environment\n",
    "!{repo_root}/setup_environment.sh --lightning --validate-only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration Setup\n",
    "\n",
    "EpiBERT uses YAML configuration files to manage all parameters. Let's create example configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Data processing configuration\n",
    "data_config = {\n",
    "    'input': {\n",
    "        'sample_name': 'demo_sample',\n",
    "        'atac_bam': 'data/raw/demo.atac.bam',\n",
    "        'rampage_bam': 'data/raw/demo.rampage.bam'  # optional for fine-tuning\n",
    "    },\n",
    "    'reference': {\n",
    "        'genome_fasta': 'reference/hg38.fa',\n",
    "        'chrom_sizes': 'reference/hg38.chrom.sizes',\n",
    "        'blacklist': 'reference/hg38-blacklist.v2.bed',\n",
    "        'motif_database': 'reference/JASPAR2022_CORE_vertebrates.meme'\n",
    "    },\n",
    "    'output': {\n",
    "        'base_dir': 'data/processed'\n",
    "    },\n",
    "    'processing': {\n",
    "        'peak_calling': {\n",
    "            'qvalue': 0.01,\n",
    "            'shift': -75,\n",
    "            'extsize': 150\n",
    "        },\n",
    "        'signal_tracks': {\n",
    "            'bin_size': 128,\n",
    "            'normalize': True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save data configuration\n",
    "with open(repo_root / 'demo_data_config.yaml', 'w') as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"Data configuration saved to demo_data_config.yaml\")\n",
    "print(yaml.dump(data_config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_config = {\n",
    "    'data': {\n",
    "        'train_data': 'data/processed/train',\n",
    "        'valid_data': 'data/processed/valid',\n",
    "        'test_data': 'data/processed/test'\n",
    "    },\n",
    "    'model': {\n",
    "        'type': 'pretraining',  # or 'finetuning'\n",
    "        'input_length': 524288,\n",
    "        'output_length': 4096,\n",
    "        'final_output_length': 4092\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 4,\n",
    "        'learning_rate': 0.0001,\n",
    "        'max_epochs': 100,\n",
    "        'patience': 10,\n",
    "        'gradient_clip_val': 1.0\n",
    "    },\n",
    "    'logging': {\n",
    "        'wandb_project': 'epibert_demo',\n",
    "        'wandb_entity': 'your_username',\n",
    "        'log_dir': 'logs'\n",
    "    },\n",
    "    'hardware': {\n",
    "        'num_gpus': 1,\n",
    "        'num_workers': 4,\n",
    "        'precision': 'bf16'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save training configuration\n",
    "with open(repo_root / 'demo_training_config.yaml', 'w') as f:\n",
    "    yaml.dump(training_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"Training configuration saved to demo_training_config.yaml\")\n",
    "print(yaml.dump(training_config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Reference Data Download\n",
    "\n",
    "Download essential reference files needed for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download reference data (this may take several minutes)\n",
    "print(\"Downloading reference data...\")\n",
    "print(\"Note: Genome FASTA download requires user confirmation due to size (3GB)\")\n",
    "\n",
    "!{repo_root}/scripts/download_references.sh --output-dir {repo_root}/reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check downloaded reference files\n",
    "import os\n",
    "\n",
    "ref_dir = repo_root / 'reference'\n",
    "if ref_dir.exists():\n",
    "    print(\"Downloaded reference files:\")\n",
    "    for file in sorted(ref_dir.iterdir()):\n",
    "        if file.is_file():\n",
    "            size = file.stat().st_size / (1024*1024)  # Size in MB\n",
    "            print(f\"  {file.name}: {size:.1f} MB\")\nelse:\n",
    "    print(\"Reference directory not found. Run the download script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Processing (Example)\n",
    "\n",
    "**Note**: This step requires actual BAM files. For demonstration, we'll show the commands without executing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the command you would run with real data:\n",
    "data_processing_cmd = f\"{repo_root}/data_processing/run_pipeline.sh -c {repo_root}/demo_data_config.yaml\"\n",
    "\n",
    "print(\"Data processing command:\")\n",
    "print(data_processing_cmd)\n",
    "print()\n",
    "print(\"This command would:\")\n",
    "print(\"1. Process ATAC-seq BAM to fragments\")\n",
    "print(\"2. Generate signal tracks\")\n",
    "print(\"3. Call peaks with MACS2\")\n",
    "print(\"4. Compute motif enrichments\")\n",
    "print(\"5. Create training datasets\")\n",
    "\n",
    "# Uncomment to run with real data:\n",
    "# !{data_processing_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Architecture Overview\n",
    "\n",
    "Let's examine the EpiBERT Lightning model architecture and parameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lightning model\n",
    "try:\n",
    "    from lightning_transfer.epibert_lightning import EpiBERTLightning\n",
    "    \n",
    "    # Create pretraining model\n",
    "    print(\"=== EpiBERT Pretraining Model Architecture ===\")\n",
    "    model_pretrain = EpiBERTLightning(model_type=\"pretraining\")\n",
    "    print(f\"Model type: {model_pretrain.model_type}\")\n",
    "    print(f\"Number of attention heads: {model_pretrain.num_heads}\")\n",
    "    print(f\"Number of transformer layers: {model_pretrain.num_transformer_layers}\")\n",
    "    print(f\"Model dimension (d_model): {model_pretrain.d_model}\")\n",
    "    print(f\"Sequence filters: {model_pretrain.filter_list_seq}\")\n",
    "    print(f\"ATAC filters: {model_pretrain.filter_list_atac}\")\n",
    "    print(f\"Dropout: {model_pretrain.dropout}\")\n",
    "    print(f\"Pointwise dropout: {model_pretrain.pointwise_dropout}\")\n",
    "    \n",
    "    print(\"\\n=== EpiBERT Fine-tuning Model Architecture ===\")\n",
    "    model_finetune = EpiBERTLightning(model_type=\"finetuning\")\n",
    "    print(f\"Model type: {model_finetune.model_type}\")\n",
    "    print(f\"Number of attention heads: {model_finetune.num_heads}\")\n",
    "    print(f\"Number of transformer layers: {model_finetune.num_transformer_layers}\")\n",
    "    print(f\"Model dimension (d_model): {model_finetune.d_model}\")\n",
    "    print(f\"Sequence filters: {model_finetune.filter_list_seq}\")\n",
    "    print(f\"ATAC filters: {model_finetune.filter_list_atac}\")\n",
    "    print(f\"Dropout: {model_finetune.dropout}\")\n",
    "    print(f\"Pointwise dropout: {model_finetune.pointwise_dropout}\")\n",
    "    \n",
    "    # Show parameter count\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nPretraining model parameters: {count_parameters(model_pretrain):,}\")\n",
    "    print(f\"Fine-tuning model parameters: {count_parameters(model_finetune):,}\")\n",
    "    \nexcept ImportError as e:\n",
    "    print(f\"Lightning dependencies not available: {e}\")\n",
    "    print(\"Install with: pip install -r lightning_transfer/requirements_lightning.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training (Example)\n",
    "\n",
    "**Note**: Training requires processed data. We'll show the training command and setup without executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training command\n",
    "training_cmd = f\"{repo_root}/scripts/train_model.sh --config {repo_root}/demo_training_config.yaml --lightning\"\n",
    "\n",
    "print(\"Training command:\")\n",
    "print(training_cmd)\n",
    "print()\n",
    "print(\"This command would:\")\n",
    "print(\"1. Validate configuration and data paths\")\n",
    "print(\"2. Initialize Lightning trainer with callbacks\")\n",
    "print(\"3. Set up logging (W&B, TensorBoard)\")\n",
    "print(\"4. Train model with early stopping\")\n",
    "print(\"5. Save best checkpoint\")\n",
    "\n",
    "# Show what the training would look like\n",
    "print(\"\\nExample training output:\")\n",
    "print(\"\"\"\n",
    "Epoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [15:23<00:00, 1.35it/s, loss=0.234, v_num=0]\n",
    "Epoch 2/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [15:18<00:00, 1.36it/s, loss=0.198, v_num=0]\n",
    "...\n",
    "Early stopping triggered. Best model saved to: models/checkpoints/epoch_15.ckpt\n",
    "\"\"\")\n",
    "\n",
    "# Uncomment to run actual training (requires processed data):\n",
    "# !{training_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model Evaluation (Example)\n",
    "\n",
    "Demonstrate the comprehensive evaluation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation tools\n",
    "try:\n",
    "    sys.path.append(str(repo_root / 'scripts'))\n",
    "    from evaluate_model import EpiBERTEvaluator\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = EpiBERTEvaluator(implementation=\"lightning\")\n",
    "    \n",
    "    print(\"EpiBERT Evaluator initialized successfully\")\n",
    "    print(\"\\nAvailable evaluation metrics:\")\n",
    "    print(\"- Correlation metrics: Pearson, Spearman (global and per-sample)\")\n",
    "    print(\"- Regression metrics: MSE, MAE, RMSE, explained variance\")\n",
    "    print(\"- Peak prediction: ROC-AUC, PR-AUC, precision, recall, F1\")\n",
    "    print(\"- Visualization: Scatter plots, residuals, distributions, Q-Q plots\")\n",
    "    \nexcept ImportError as e:\n",
    "    print(f\"Evaluation dependencies not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example evaluation command (would run with real model and data)\n",
    "evaluation_cmd = f\"\"\"python3 {repo_root}/scripts/evaluate_model.py \\\\\n",
    "    --model_path models/checkpoints/best_model.ckpt \\\\\n",
    "    --test_data data/processed/test.h5 \\\\\n",
    "    --implementation lightning \\\\\n",
    "    --model_type pretraining \\\\\n",
    "    --output_dir results/evaluation\"\"\"\n",
    "\n",
    "print(\"Evaluation command:\")\n",
    "print(evaluation_cmd)\n",
    "print()\n",
    "print(\"This would generate:\")\n",
    "print(\"1. Comprehensive metrics JSON file\")\n",
    "print(\"2. Evaluation plots (scatter, residuals, distributions)\")\n",
    "print(\"3. Per-sample correlation analysis\")\n",
    "print(\"4. Peak prediction performance assessment\")\n",
    "\n",
    "# Show example metrics output\n",
    "print(\"\\nExample evaluation metrics:\")\n",
    "example_metrics = {\n",
    "    \"global_pearson_r\": 0.847,\n",
    "    \"global_spearman_r\": 0.832,\n",
    "    \"mean_sample_pearson\": 0.756,\n",
    "    \"std_sample_pearson\": 0.089,\n",
    "    \"mse\": 0.045,\n",
    "    \"mae\": 0.163,\n",
    "    \"explained_variance\": 0.718,\n",
    "    \"roc_auc\": 0.923,\n",
    "    \"pr_auc\": 0.845,\n",
    "    \"precision\": 0.784,\n",
    "    \"recall\": 0.692,\n",
    "    \"f1_score\": 0.735\n",
    "}\n",
    "\n",
    "for metric, value in example_metrics.items():\n",
    "    print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Complete Workflow Automation\n",
    "\n",
    "The master workflow script can automate the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow command\n",
    "workflow_cmd = f\"{repo_root}/run_complete_workflow.sh --lightning --dry-run\"\n",
    "\n",
    "print(\"Complete workflow command (dry run):\")\n",
    "print(workflow_cmd)\n",
    "print()\n",
    "print(\"To run the complete workflow with real data:\")\n",
    "print(f\"{repo_root}/run_complete_workflow.sh --lightning\")\n",
    "\n",
    "# Run dry run to show workflow overview\n",
    "!{workflow_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Model Usage for Predictions\n",
    "\n",
    "Example of using a trained model for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction code (would work with trained model)\n",
    "prediction_example = '''\n",
    "# Load trained model\n",
    "from lightning_transfer.epibert_lightning import EpiBERTLightning\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load model from checkpoint\n",
    "model = EpiBERTLightning.load_from_checkpoint(\"models/checkpoints/best_model.ckpt\")\n",
    "model.eval()\n",
    "\n",
    "# Prepare input data\n",
    "# Input should be shape: (batch_size, input_length, num_features)\n",
    "# For EpiBERT: (batch_size, 524288, 4) for sequence one-hot encoding\n",
    "input_sequences = torch.randn(2, 524288, 4)  # Example random input\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_sequences)\n",
    "\n",
    "print(f\"Input shape: {input_sequences.shape}\")\n",
    "print(f\"Output shape: {predictions.shape}\")\n",
    "print(f\"Predictions range: [{predictions.min():.3f}, {predictions.max():.3f}]\")\n",
    "'''\n",
    "\n",
    "print(\"Example prediction code:\")\n",
    "print(prediction_example)\n",
    "\n",
    "# Try to import Lightning modules to check availability\n",
    "try:\n",
    "    import torch\n",
    "    import pytorch_lightning as pl\n",
    "    from lightning_transfer.epibert_lightning import EpiBERTLightning\n",
    "    \n",
    "    print(\"\\nâœ… Lightning modules available for prediction\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"Lightning version: {pl.__version__}\")\n",
    "    \nexcept ImportError as e:\n",
    "    print(f\"\\nâŒ Lightning modules not available: {e}\")\n",
    "    print(\"Install with: pip install -r lightning_transfer/requirements_lightning.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete EpiBERT workflow:\n",
    "\n",
    "### âœ… What we covered:\n",
    "1. **Environment validation** - Checking dependencies and tools\n",
    "2. **Configuration setup** - Creating YAML config files for data and training\n",
    "3. **Reference data download** - Getting genome files and annotations\n",
    "4. **Model architecture** - Understanding Lightning implementation parameters\n",
    "5. **Training workflow** - Command structure and expected outputs\n",
    "6. **Evaluation system** - Comprehensive metrics and visualization\n",
    "7. **Complete automation** - Master workflow script usage\n",
    "8. **Model usage** - Making predictions with trained models\n",
    "\n",
    "### ðŸš€ Next steps for real usage:\n",
    "1. **Prepare your data**: Get ATAC-seq (and optionally RAMPAGE-seq) BAM files\n",
    "2. **Run setup**: `./setup_environment.sh --lightning --install-deps`\n",
    "3. **Download references**: `./scripts/download_references.sh`\n",
    "4. **Configure workflow**: Edit the YAML config files with your data paths\n",
    "5. **Run pipeline**: `./run_complete_workflow.sh --lightning`\n",
    "\n",
    "### ðŸ“š Additional resources:\n",
    "- **Data processing documentation**: `data_processing/README.md`\n",
    "- **Lightning implementation details**: `lightning_transfer/README.md`\n",
    "- **Example notebooks**: Other notebooks in `example_usage/`\n",
    "- **Complete documentation**: `README.md`\n",
    "\n",
    "### ðŸ’¡ Tips for success:\n",
    "- Start with a small test dataset to validate the pipeline\n",
    "- Monitor GPU memory usage and adjust batch size if needed\n",
    "- Use Weights & Biases for experiment tracking\n",
    "- Save intermediate results in case of interruptions\n",
    "- Review evaluation metrics to ensure model quality\n",
    "\n",
    "**Happy training! ðŸ§¬ðŸ¤–**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}