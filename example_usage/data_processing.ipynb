{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bddf10-0091-4e81-a0bd-432dd45e019e",
   "metadata": {},
   "source": [
    "# input processing for running EpiBERT on sample data\n",
    "EpiBERT requires (1) an input motif enrichment file generated using the Simple Enrichment Analysis (SEA) tool from the MEME-suite and (2) a processed ATAC-seq read ends file\n",
    "\n",
    "You will need working installations of bedtools, samtools, and MEME to process the data as described below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0c637a-2773-4901-aea2-5b6c11e05b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start by creating the processed motif enrichment file for an input dataset\n",
    "# as an example to start we can download IDR thresholded peaks for K562\n",
    "!wget -q https://www.encodeproject.org/files/ENCFF135AEX/@@download/ENCFF135AEX.bed.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63448ede-b6ae-46e6-8a42-466f9aec03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that you have hg38 fasta downloaded (can get from here https://www.encodeproject.org/references/ENCSR938RZZ/)\n",
    "# ensure that the fasta file is indexed\n",
    "!samtools faidx hg38_erccpatch.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c982e6c3-a13e-411b-b050-d30cb46d27b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "#now we will take the top 50000 peaks and extract the 128 bp around the peak center\n",
    "!zcat ENCFF135AEX.bed.gz | sort -k5,5nr | head -n 50000 | awk '{OFS = \"\\t\" }{print $1,$2+$10-64,$2+$10+64}' | sort -k1,1 -k2,2n > ENCFF135AEX.sorted.peaks.bed\n",
    "!bedtools getfasta -fi hg38_erccpatch.fa -bed ENCFF135AEX.sorted.peaks.bed > ENCFF135AEX.peaks.fasta\n",
    "!bedtools getfasta -fi hg38_erccpatch.fa -bed all_peaks_merged.counts.shared.centered.bed > bg_peaks.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd19d22-6dfe-43cd-aa36-943e4dbfb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run simple enrichment analysis from MEME using the 693 consensus PWMs from https://resources.altius.org/~jvierstra/projects/motif-clustering-v2.0beta/\n",
    "!/home/jupyter/meme-5.5.6/src/sea --p ENCFF135AEX.peaks.fasta --m consensus_pwms.meme --n bg_peaks.fasta --thresh 50000.0 --verbosity 1\n",
    "!mv sea_out/sea.tsv ENCFF135AEX.motifs.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d73b97-e386-4add-b29d-b488d784d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we prepare the BAM file\n",
    "# we can download the processed BAM file from encode for K562 (https://www.encodeproject.org/files/ENCFF534DCE/@@download/ENCFF534DCE.bam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81be3699-6291-42d5-b3f4-434f7e953ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we sort the bam\n",
    "#!samtools sort -n ENCFF534DCE.bam -o ENCFF534DCE.sorted.bam\n",
    "# then for each pair extract the 5 and 3' cut sites and make the required Tn5 adjustmenet\n",
    "!bedtools bamtobed -i ENCFF534DCE.sorted.bam -bedpe | \\\n",
    "        awk '$1 == $4' | \\\n",
    "        awk '$8 >= 20' | \\\n",
    "        awk -v OFS=\"\\t\" '{if($9 == \"+\"){print $1,$2+4,$6-5}else if($9==\"-\"){print $1,$5+4,$3-5}}' | \\\n",
    "        awk -v OFS=\"\\t\" '{if($3<$2){print $1,$3,$2}else if($3>$2){print $1,$2,$3}}' | \\\n",
    "        awk '$3-$2 > 0' | \\\n",
    "        sort -k1,1 -k2,2n | gzip > K562.bed.gz\n",
    "\n",
    "\n",
    "## get scale factor which is 111.279\n",
    "!zcat K562.bed.gz | wc -l | awk '{ print $1 / 1000000.0 }' > K562.num_fragments.out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e7aed86-7f08-46dd-a780-6e6275ff7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zcat K562.bed.gz | awk '{OFS=\"\\t\"}{print $1,$2+4,$2+4+1 }' | gzip > fwd.bed.gz # get fwd strand\n",
    "!zcat K562.bed.gz | awk '{OFS=\"\\t\"}{ print $1,$3-5,$3-5+1 }' |gzip > rev.bed.gz # get reverse strand\n",
    "!zcat fwd.bed.gz rev.bed.gz | sort -k1,1 -k2,2n > HG_K562.bed.temp # concat\n",
    "\n",
    "# turn into bedgraph (get 10bp around each insertion site, then scale to reads per 20 million -> 20/ 111.279  = 0.1797\n",
    "# make sure you provide a genome file for bedtools genomcov \n",
    "!cat HG_K562.bed.temp | awk '{ OFS=\"\\t\" } {print $1,$2-5,$3+5}' | \\\n",
    "                    grep -v 'KI\\|GL\\|EBV\\|chrM\\|chrMT\\|K\\|J' | \\\n",
    "                    awk '$2 >= 0' | sort -k1,1 -k2,2n | \\\n",
    "                    bedtools genomecov -i - -g hg38.genome -scale 0.1797 -bg | sort -k1,1 -k2,2n > K562.bedgraph\n",
    "\n",
    "!mv K562.bedgraph K562.adjust.bed # rename since tabix will throw an error if bedgraph named \n",
    "!bgzip K562.adjust.bed # bgzip for tabix \n",
    "!tabix K562.adjust.bed.gz # tabix index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1858b49-e34f-41c1-b0ac-0bfdd12d0d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
