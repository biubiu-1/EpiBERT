{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bddf10-0091-4e81-a0bd-432dd45e019e",
   "metadata": {},
   "source": [
    "# input processing for running EpiBERT on sample data\n",
    "EpiBERT requires (1) an input motif enrichment file generated using the Simple Enrichment Analysis (SEA) tool from the MEME-suite and (2) a processed ATAC-seq read ends file\n",
    "\n",
    "You will need working installations of bedtools, samtools, and MEME to process the data as described below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0c637a-2773-4901-aea2-5b6c11e05b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start by creating the processed motif enrichment file for an input dataset\n",
    "# as an example to start we can download IDR thresholded peaks for K562\n",
    "!wget -q https://www.encodeproject.org/files/ENCFF135AEX/@@download/ENCFF135AEX.bed.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63448ede-b6ae-46e6-8a42-466f9aec03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that you have hg38 fasta downloaded (can get from here https://www.encodeproject.org/references/ENCSR938RZZ/)\n",
    "# ensure that the fasta file is indexed\n",
    "!samtools faidx hg38_erccpatch.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c982e6c3-a13e-411b-b050-d30cb46d27b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort: write failed: 'standard output': Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "#now we will take the top 50000 peaks and extract the 128 bp around the peak center\n",
    "!zcat ENCFF135AEX.bed.gz | sort -k5,5nr | head -n 50000 | awk '{OFS = \"\\t\" }{print $1,$2+$10-64,$2+$10+64}' | sort -k1,1 -k2,2n > ENCFF135AEX.sorted.peaks.bed\n",
    "!bedtools getfasta -fi hg38_erccpatch.fa -bed ENCFF135AEX.sorted.peaks.bed > ENCFF135AEX.peaks.fasta\n",
    "!bedtools getfasta -fi hg38_erccpatch.fa -bed all_peaks_merged.counts.shared.centered.bed > bg_peaks.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd19d22-6dfe-43cd-aa36-943e4dbfb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run simple enrichment analysis from MEME using the consensus PWMs from https://resources.altius.org/~jvierstra/projects/motif-clustering-v2.0beta/\n",
    "!/home/jupyter/meme-5.5.6/src/sea --p ENCFF135AEX.peaks.fasta --m consensus_pwms.meme --n bg_peaks.fasta --thresh 1.0 --verbosity 1\n",
    "!mv sea_out/sea.tsv ENCFF135AEX.motifs.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d73b97-e386-4add-b29d-b488d784d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we prepare the BAM file\n",
    "# we can download the processed BAM file from encode for K562 (https://www.encodeproject.org/files/ENCFF534DCE/@@download/ENCFF534DCE.bam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81be3699-6291-42d5-b3f4-434f7e953ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bam_sort_core] merging from 30 files and 1 in-memory blocks...\n"
     ]
    }
   ],
   "source": [
    "# first we sort the bam\n",
    "!samtools sort -n ENCFF534DCE.bam -o ENCFF534DCE.sorted.bam\n",
    "# then for each pair extract the 5 and 3' cut sites and make the required Tn5 adjustmenet\n",
    "!bedtools bamtobed -i ENCFF534DCE.sorted.bam -bedpe | awk '$1 == $4' | awk '$8 >= 20' | awk -v OFS=\"\\t\" '{if($9 == \"+\"){print $1,$2+4,$6-5}else if($9==\"-\"){print $1,$5+4,$3-5}}' | awk -v OFS=\"\\t\" '{if($3<$2){print $1,$3,$2}else if($3>$2){print $1,$2,$3}}' | awk '$3-$2 > 0' | gzip > K562.bed.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689282c-3e58-4ac9-8101-d989d91b63d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
